{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Cross-Validation and the Bootstrap\n",
    "Reference: James, G., Witten, D., Hastie, T., Tibshirani, R.,, Taylor, J. (2023). An Introduction to Statistical Learning with Applications in Python. Cham: Springer. ISBN: 978-3-031-38746-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS,\n",
    "                        summarize,\n",
    "                        poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "from sklearn.model_selection import \\\n",
    "                        (cross_validate,\n",
    "                        KFold,\n",
    "                        ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Validation Set Approach\n",
    "Use the function `train_test_split()` to split the data into training train_test_\n",
    "and validation sets. As there are 392 observations, we split into two equal\n",
    "sets of size 196 using the argument test_size = `196`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto = load_data('Auto')\n",
    "Auto_train, Auto_test = train_test_split(Auto, test_size = 196, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit the model by linear regression using observations from training set `Auto_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>39.9055</td>\n",
       "      <td>1.009</td>\n",
       "      <td>39.537</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horsepower</th>\n",
       "      <td>-0.1563</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-17.333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               coef  std err       t  P>|t|\n",
       "intercept   39.9055    1.009  39.537    0.0\n",
       "horsepower  -0.1563    0.009 -17.333    0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_mm = MS(['horsepower'])\n",
    "X_train = hp_mm.fit_transform(Auto_train)\n",
    "y_train = Auto_train['mpg']\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results = model.fit()\n",
    "summarize(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the `predict()` method of results evaluated on the model matrix for this model created using the validation data set. We also calculate the validation MSE of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.61661706966988"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = hp_mm.transform(Auto_test)\n",
    "y_test = Auto_test['mpg']\n",
    "test_pred = results.predict(X_test)\n",
    "np.mean((y_test - test_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimate for the validation MSE of the linear regression fit is `23.62`.\n",
    "Also estimation of the validation error for higher-degree polynomial regressions is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMSE(terms, response, train, test):\n",
    "    mm = MS(terms)\n",
    "    X_train = mm.fit_transform(train)\n",
    "    y_train = train[response]\n",
    "    \n",
    "    X_test = mm.transform(test)\n",
    "    y_test = test[response]\n",
    "    \n",
    "    results = sm.OLS(y_train, X_train).fit()\n",
    "    test_pred = results.predict(X_test)\n",
    "    \n",
    "    return np.mean((y_test - test_pred)**2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707, 18.76303135, 18.79694163])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)], 'mpg', Auto_train, Auto_test)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These error rates are `23.62`, `18.76`, and `18.80`, respectively. If we choose a\n",
    "different training/validation split instead, then we can expect somewhat\n",
    "different errors on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.75540796, 16.94510676, 16.97437833])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Auto_train, Auto_test = train_test_split(Auto, test_size = 196, random_state = 3)\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1, 4)):\n",
    "    MSE[idx] = evalMSE([poly('horsepower', degree)], 'mpg', Auto_train, Auto_test)\n",
    "MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the validation set error rates for the models with linear,quadratic, and cubic terms are `20.76`, `16.95`, and `16.97`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.231513517929212"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model = sklearn_sm(sm.OLS, MS(['horsepower']))\n",
    "X, Y = Auto.drop(columns=['mpg']), Auto['mpg']\n",
    "cv_results = cross_validate(hp_model,\n",
    "                        X,\n",
    "                        Y,\n",
    "cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results['test_score'])\n",
    "cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments to cross_validate() are as follows: an object with the appropriate `fit()`, `predict()`, and `score()` methods, an array of features  `X` and\n",
    "a response `Y`. We also included an additional argument `cv` to `cross_validate()`;\n",
    "specifying an integer K results in K-fold cross-validation. We have provided\n",
    "a value corresponding to the total number of observations, which results in leave-one-out cross-validation (LOOCV). The `cross_validate()` function produces a dictionary with several components; we simply want the cross-validated test score here (MSE), which is estimated to be `24.23`.\n",
    "\n",
    "We can repeat this procedure for increasingly complex polynomial fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.42443031, 19.03320428])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "H = np.array(Auto['horsepower'])\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(M,\n",
    "                        X,\n",
    "                        Y,\n",
    "    cv=Auto.shape[0])\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-degree polynomials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  7],\n",
       "       [ 7,  9],\n",
       "       [11, 13]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([3, 5, 9])\n",
    "B = np.array([2, 4])\n",
    "np.add.outer(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CV example above, we used `K = n`. Here we use `KFold()` to partition the data into `K = 10` random groups. We `KFold()` use random_state to set a random seed and initialize a vector `cv_error` in which we will store the CV errors corresponding to the polynomial fits of degrees one to five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848404, 19.13722016])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv = KFold( n_splits=10,\n",
    "            shuffle=True,\n",
    "            random_state=0) # use same splits for each degree\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d+1))\n",
    "    M_CV = cross_validate(  M,\n",
    "                            X,\n",
    "                            Y,\n",
    "                            cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV['test_score'])\n",
    "cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the computation time is much shorter than that of LOOCV.\n",
    "(In principle, the computation time for LOOCV for a least squares linear\n",
    "model should be faster than for K-fold CV)\n",
    "\n",
    "The `cross_validate()` function is flexible and can take different splitting\n",
    "mechanisms as an argument. For instance, one can use the `ShuffleSplit()` \n",
    "funtion to implement the validation set approach just as easily as K-fold\n",
    "cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit( n_splits=1,\n",
    "                        test_size=196,\n",
    "                        random_state=0)\n",
    "results = cross_validate(  hp_model,\n",
    "                        Auto.drop(['mpg'], axis=1),\n",
    "                        Auto['mpg'],\n",
    "                        cv=validation)\n",
    "results['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can estimate the variability in the test error by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.802232661034164, 1.4218450941091847)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(  n_splits=10,\n",
    "                            test_size=196,\n",
    "                            random_state=0)\n",
    "results = cross_validate(   hp_model,\n",
    "                            Auto.drop(['mpg'], axis=1),\n",
    "                            Auto['mpg'],\n",
    "                            cv=validation)\n",
    "results['test_score'].mean(), results['test_score'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this standard deviation is not a valid estimate of the sampling variability of the mean test score or the individual scores, since the\n",
    "randomly-selected training samples overlap and hence introduce correlations. \n",
    "\n",
    "### The Bootstrap\n",
    "No complicated mathematical calculations are required for Bootstrap method.\n",
    "The `Portfolio` data set is used.The goal is to estimate the sampling variance of the parameter α. We will create a function alpha_func(), which takes as input a dataframe D assumed to have columns X and Y, as well as a vector idx indicating which\n",
    "observations should be used to estimate α. The function then outputs the estimate for based on the selected observations.\n",
    "#### Estimating the Accuracy of a Statistic of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio = load_data('Portfolio')\n",
    "def alpha_func(D, idx):\n",
    " cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False)\n",
    " return ((cov_[1,1]- cov_[0,1]) /\n",
    " (cov_[0,0]+cov_[1,1]-2*cov_[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns an estimate for based on applying the minimum variance formula  to the observations indexed by the argument `idx`. For\n",
    " instance, the following command estimates using all 100 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57583207459283"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_func(Portfolio, range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next we randomly select 100 observations from `range(100)`, with replacement. This is equivalent to constructing a new bootstrap data set and\n",
    " recomputing ˆbased on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6074452469619004"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "alpha_func(Portfolio, rng.choice(100, 100, replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process can be generalized to create a simple function `boot_SE()` for\n",
    "computing the bootstrap standard error for arbitrary functions that take\n",
    "only a data frame as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_SE(func, D, n=None, B=1000, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0, 0\n",
    "    n = n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index, n, replace=True)\n",
    "        value = func(D, idx)\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B- (first_ / B)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use our function to evaluate the accuracy of our estimate of using B = 1,000 bootstrap replications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09118176521277699"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_SE = boot_SE(alpha_func, Portfolio, B=1000, seed=0)\n",
    "alpha_SE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output shows that the bootstrap estimate for SE(ˆα) is 0.0912.\n",
    "\n",
    "####  Estimating the Accuracy of a Linear Regression Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the bootstrap approach in order to assess the variability of the estimates for B0 and B1, the intercept and slope terms for the linear regres\n",
    "sion model that uses horsepower to predict mpg in the Auto data set. \n",
    "\n",
    "To use our `boot_SE()` function, we must write a function (its first argu\n",
    "ment) that takes a data frame `D` and indices `idx` as its only arguments. But\n",
    "here we want to bootstrap a specific regression model, specified by a model\n",
    "formula and data. \n",
    "\n",
    "We start by writing a generic function `boot_OLS()` for bootstrapping a\n",
    "regression model that takes a formula to define the corresponding regres\n",
    "sion. We use the `clone()` function to make a copy of the formula that can \n",
    "be refit to the new dataframe. This means that any derived features such\n",
    "as those defined by` poly()` (which we will see shortly), will be re-fit on the\n",
    "resampled data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_ = D.loc[idx]\n",
    "    Y_ = D_[response]\n",
    "    X_ = clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_, X_).fit().params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not quite what is needed as the first argument to `boot_SE()`. The first\n",
    "two arguments which specify the model will not change in the bootstrap\n",
    "process, and we would like to freeze them. The function `partial()` from the \n",
    "`functools` module does precisely this: it takes a function as an argument,\n",
    "and freezes some of its arguments, starting from the left. We use it to freeze\n",
    "the first two model-formula arguments of `boot_OLS()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.88064456, -0.1567849 ],\n",
       "       [38.73298691, -0.14699495],\n",
       "       [38.31734657, -0.14442683],\n",
       "       [39.91446826, -0.15782234],\n",
       "       [39.43349349, -0.15072702],\n",
       "       [40.36629857, -0.15912217],\n",
       "       [39.62334517, -0.15449117],\n",
       "       [39.0580588 , -0.14952908],\n",
       "       [38.66688437, -0.14521037],\n",
       "       [39.64280792, -0.15555698]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# Reset index to ensure compatibility\n",
    "Auto = Auto.reset_index(drop=True)\n",
    "\n",
    "results = np.array([\n",
    "    hp_func(Auto, rng.choice(Auto.index, 392, replace=True)) for _ in range(10)\n",
    "])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the boot_SE() function to compute the standard errors of\n",
    " 1,000 bootstrap estimates for the intercept and slope terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.848807\n",
       "horsepower    0.007352\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_se = boot_SE(hp_func,\n",
    "                Auto,\n",
    "                B=1000,\n",
    "                seed=10)\n",
    "hp_se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the bootstrap estimate for $SE(\\hat{\\beta}_0)$ is 0.85, and that the bootstrap estimate for $SE(\\hat{\\beta}_1)$ is 0.0074.\n",
    "\n",
    "Standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.717\n",
       "horsepower    0.006\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model.fit(Auto, Auto['mpg'])\n",
    "model_se = summarize(hp_model.results_)['std err']\n",
    "model_se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error estimates for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are `0.717` for the intercept and `0.006` for the slope.  These are somewhat different from the estimates obtained using the bootstrap.\n",
    "\n",
    " Below we compute the bootstrap standard error estimates and the stan\n",
    "dard linear regression estimates that result from fitting the quadratic model\n",
    " to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  2.067840\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.033019\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000120\n",
       "dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quad_model = MS([poly('horsepower', 2, raw=True)])\n",
    "quad_func = partial(boot_OLS,\n",
    "                    quad_model,\n",
    "                    'mpg')\n",
    "boot_SE(quad_func, Auto, B=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the results to the standard errors computed using `sm.OLS()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.800\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.031\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = sm.OLS(Auto['mpg'],\n",
    "quad_model.fit_transform(Auto))\n",
    "summarize(M.fit())['std err']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
